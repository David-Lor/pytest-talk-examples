{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Getting started\n",
    "\n",
    "- PyTest buscará todos los archivos de tests recursivamente desde el directorio actual\n",
    "- El nombre de los módulos (archivos .py) debe empezar por `test_` (excepto si indicamos el archivo .py manualmente, como en el siguiente ejemplo)\n",
    "- Un test por función: cada nombre de función debe empezar por `test_`\n",
    "- Podemos saltar tests con el decorador `@pytest.mark.skip(\"blablabla\")` (la línea `import pytest` está por este motivo, sino no sería necesaria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m\"\"\"A1 - TEST with functions in module level, and testing exceptions\u001b[39;49;00m\r\n",
      "\u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpytest\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest_sum\u001b[39;49;00m():\r\n",
      "    \u001b[34massert\u001b[39;49;00m \u001b[34m2\u001b[39;49;00m + \u001b[34m2\u001b[39;49;00m == \u001b[34m4\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest_divide\u001b[39;49;00m():\r\n",
      "    \u001b[34massert\u001b[39;49;00m \u001b[34m4\u001b[39;49;00m / \u001b[34m2\u001b[39;49;00m == \u001b[34m2\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest_divide_zero\u001b[39;49;00m():\r\n",
      "    \u001b[33m\"\"\"This test will fail if the code inside \"with pytest.raises(...)\" does not raise the given exception\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[34mwith\u001b[39;49;00m pytest.raises(\u001b[36mZeroDivisionError\u001b[39;49;00m):\r\n",
      "        \u001b[34massert\u001b[39;49;00m \u001b[34m2\u001b[39;49;00m / \u001b[34m0\u001b[39;49;00m == \u001b[34m0\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[90m@pytest\u001b[39;49;00m.mark.skip(\u001b[33m\"\u001b[39;49;00m\u001b[33mDon\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mt want to run this right now!\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest_skip\u001b[39;49;00m():\r\n",
      "    \u001b[33m\"\"\"This test is marked as \"skip\" and will not run (but shown on results)\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[34massert\u001b[39;49;00m \u001b[34m2\u001b[39;49;00m / \u001b[34m0\u001b[39;49;00m == \u001b[34m0\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mthis_test_will_not_run\u001b[39;49;00m():\r\n",
      "    \u001b[33m\"\"\"All test modules (filenames) and methods (function names) must start with \"test_\"\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[34massert\u001b[39;49;00m \u001b[34mTrue\u001b[39;49;00m \u001b[35mis\u001b[39;49;00m \u001b[34mFalse\u001b[39;49;00m\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize examples/A_helloworld/test_A1_helloworld.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform linux -- Python 3.7.4, pytest-5.3.5, py-1.8.1, pluggy-0.13.1 -- /home/david/.miniconda3/bin/python\r\n",
      "cachedir: .pytest_cache\r\n",
      "rootdir: /home/david/Nextcloud/Codigos/Python/2Presentaciones/pyTest\r\n",
      "\u001b[1mcollecting ... \u001b[0m\u001b[1m\r",
      "collected 4 items                                                              \u001b[0m\r\n",
      "\r\n",
      "examples/A_helloworld/test_A1_helloworld.py::test_sum \u001b[32mPASSED\u001b[0m\r\n",
      "examples/A_helloworld/test_A1_helloworld.py::test_divide \u001b[32mPASSED\u001b[0m\r\n",
      "examples/A_helloworld/test_A1_helloworld.py::test_divide_zero \u001b[32mPASSED\u001b[0m\r\n",
      "examples/A_helloworld/test_A1_helloworld.py::test_skip \u001b[33mSKIPPED\u001b[0m\r\n",
      "\r\n",
      "\u001b[32m========================= \u001b[32m\u001b[1m3 passed\u001b[0m, \u001b[33m1 skipped\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m =========================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pytest -sv examples/A_helloworld/test_A1_helloworld.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform linux -- Python 3.7.4, pytest-5.3.5, py-1.8.1, pluggy-0.13.1\r\n",
      "rootdir: /home/david/Nextcloud/Codigos/Python/2Presentaciones/pyTest\r\n",
      "\u001b[1mcollecting ... \u001b[0m\u001b[1m\r",
      "collected 9 items                                                              \u001b[0m\r\n",
      "<Module examples/A_helloworld/test_A1_helloworld.py>\r\n",
      "  <Function test_sum>\r\n",
      "  <Function test_divide>\r\n",
      "  <Function test_skip>\r\n",
      "<Module examples/A_helloworld/test_A2_failing.py>\r\n",
      "  <Function test_failing>\r\n",
      "  <Function test_failing_not_raising>\r\n",
      "<Module examples/A_helloworld/test_A3_expect_raises.py>\r\n",
      "  <Function test_expect_zerodivisionerror_raised>\r\n",
      "  <Function test_expect_zerodivisionerror_not_raised>\r\n",
      "  <Function test_expect_zerodivisionerror_raised_other>\r\n",
      "  <Function test_expect_typeerror>\r\n",
      "\r\n",
      "\u001b[33m============================ \u001b[33mno tests ran\u001b[0m\u001b[33m in 0.03s\u001b[0m\u001b[33m =============================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# Proving that test modules/functions not starting with \"test_\" won't start\n",
    "# pytest --collect-only lists existing tests\n",
    "\n",
    "!pytest --collect-only examples/A_helloworld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2) Failing tests\n",
    "\n",
    "- Cualquier test que haga saltar una excepción será fallido\n",
    "- Común utilizar `assert` para expresiones binarias (si la expresión es False lanza AssertionError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m\"\"\"A2 - TEST with functions in module level, with a failing test\u001b[39;49;00m\r\n",
      "\u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest_failing_zerodivisionerror\u001b[39;49;00m():\r\n",
      "    \u001b[34m2\u001b[39;49;00m / \u001b[34m0\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest_failing_compare_true_is_false\u001b[39;49;00m():\r\n",
      "    \u001b[34massert\u001b[39;49;00m \u001b[34mTrue\u001b[39;49;00m == \u001b[34mFalse\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest_failing_compare_numbers_equality\u001b[39;49;00m():\r\n",
      "    \u001b[34massert\u001b[39;49;00m \u001b[34m2\u001b[39;49;00m == \u001b[34m3\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest_failing_compare_lists\u001b[39;49;00m():\r\n",
      "    l1 = [\u001b[33m\"\u001b[39;49;00m\u001b[33mA\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mB\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mC\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\r\n",
      "    l2 = [\u001b[33m\"\u001b[39;49;00m\u001b[33mA\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mC\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mB\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\r\n",
      "    \u001b[34massert\u001b[39;49;00m l1 == l2\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize examples/A_helloworld/test_A2_failing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.4, pytest-5.3.5, py-1.8.1, pluggy-0.13.1 -- /home/david/.miniconda3/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/david/Nextcloud/Codigos/Python/2Presentaciones/pyTest\n",
      "collected 4 items                                                              \u001b[0m\n",
      "\n",
      "examples/A_helloworld/test_A2_failing.py::test_failing_zerodivisionerror \u001b[31mFAILED\u001b[0m\n",
      "examples/A_helloworld/test_A2_failing.py::test_failing_compare_true_is_false \u001b[31mFAILED\u001b[0m\n",
      "examples/A_helloworld/test_A2_failing.py::test_failing_compare_numbers_equality \u001b[31mFAILED\u001b[0m\n",
      "examples/A_helloworld/test_A2_failing.py::test_failing_compare_lists \u001b[31mFAILED\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m________________________ test_failing_zerodivisionerror ________________________\u001b[0m\n",
      "\n",
      "\u001b[1m    def test_failing_zerodivisionerror():\u001b[0m\n",
      "\u001b[1m>       2 / 0\u001b[0m\n",
      "\u001b[1m\u001b[31mE       ZeroDivisionError: division by zero\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mexamples/A_helloworld/test_A2_failing.py\u001b[0m:5: ZeroDivisionError\n",
      "\u001b[31m\u001b[1m______________________ test_failing_compare_true_is_false ______________________\u001b[0m\n",
      "\n",
      "\u001b[1m    def test_failing_compare_true_is_false():\u001b[0m\n",
      "\u001b[1m>       assert True == False\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert True == False\u001b[0m\n",
      "\u001b[1m\u001b[31mE         -True\u001b[0m\n",
      "\u001b[1m\u001b[31mE         +False\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mexamples/A_helloworld/test_A2_failing.py\u001b[0m:9: AssertionError\n",
      "\u001b[31m\u001b[1m____________________ test_failing_compare_numbers_equality _____________________\u001b[0m\n",
      "\n",
      "\u001b[1m    def test_failing_compare_numbers_equality():\u001b[0m\n",
      "\u001b[1m>       assert 2 == 3\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 2 == 3\u001b[0m\n",
      "\u001b[1m\u001b[31mE         -2\u001b[0m\n",
      "\u001b[1m\u001b[31mE         +3\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mexamples/A_helloworld/test_A2_failing.py\u001b[0m:13: AssertionError\n",
      "\u001b[31m\u001b[1m__________________________ test_failing_compare_lists __________________________\u001b[0m\n",
      "\n",
      "\u001b[1m    def test_failing_compare_lists():\u001b[0m\n",
      "\u001b[1m        l1 = [\"A\", \"B\", \"C\"]\u001b[0m\n",
      "\u001b[1m        l2 = [\"A\", \"C\", \"B\"]\u001b[0m\n",
      "\u001b[1m>       assert l1 == l2\u001b[0m\n",
      "\u001b[1m\u001b[31mE       AssertionError: assert ['A', 'B', 'C'] == ['A', 'C', 'B']\u001b[0m\n",
      "\u001b[1m\u001b[31mE         At index 1 diff: 'B' != 'C'\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Full diff:\u001b[0m\n",
      "\u001b[1m\u001b[31mE         - ['A', 'B', 'C']\u001b[0m\n",
      "\u001b[1m\u001b[31mE         + ['A', 'C', 'B']\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mexamples/A_helloworld/test_A2_failing.py\u001b[0m:19: AssertionError\n",
      "\u001b[31m============================== \u001b[31m\u001b[1m4 failed\u001b[0m\u001b[31m in 0.02s\u001b[0m\u001b[31m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -sv examples/A_helloworld/test_A2_failing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A3) Raises (expect exceptions)\n",
    "\n",
    "- Si queremos que una porción de código falle explícitamente, utilizamos el decorador `with pytest.raises(Exception):`, siendo Exception la clase de excepción esperada\n",
    "- El test fallará si el código dentro del decorador NO obtiene la excepción esperada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m\"\"\"A3 - TEST functions using pytest.raises\u001b[39;49;00m\r\n",
      "\u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpytest\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest_expect_zerodivisionerror_raised\u001b[39;49;00m():  \u001b[37m# passes\u001b[39;49;00m\r\n",
      "    \u001b[34mwith\u001b[39;49;00m pytest.raises(\u001b[36mZeroDivisionError\u001b[39;49;00m):\r\n",
      "        \u001b[34m2\u001b[39;49;00m / \u001b[34m0\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest_expect_zerodivisionerror_not_raised\u001b[39;49;00m():  \u001b[37m# fails\u001b[39;49;00m\r\n",
      "    \u001b[34mwith\u001b[39;49;00m pytest.raises(\u001b[36mZeroDivisionError\u001b[39;49;00m):\r\n",
      "        \u001b[34m2\u001b[39;49;00m / \u001b[34m1\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest_expect_zerodivisionerror_raised_other\u001b[39;49;00m():  \u001b[37m# fails\u001b[39;49;00m\r\n",
      "    \u001b[34mwith\u001b[39;49;00m pytest.raises(\u001b[36mZeroDivisionError\u001b[39;49;00m):\r\n",
      "        \u001b[34m2\u001b[39;49;00m / \u001b[33m\"\u001b[39;49;00m\u001b[33mnot a number\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest_expect_typeerror_raised\u001b[39;49;00m():  \u001b[37m# passes\u001b[39;49;00m\r\n",
      "    \u001b[34mwith\u001b[39;49;00m pytest.raises(\u001b[36mTypeError\u001b[39;49;00m):\r\n",
      "        \u001b[34m2\u001b[39;49;00m / \u001b[33m\"\u001b[39;49;00m\u001b[33mnot a number\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize examples/A_helloworld/test_A3_expect_raises.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform linux -- Python 3.7.4, pytest-5.3.5, py-1.8.1, pluggy-0.13.1 -- /home/david/.miniconda3/bin/python\r\n",
      "cachedir: .pytest_cache\r\n",
      "rootdir: /home/david/Nextcloud/Codigos/Python/2Presentaciones/pyTest\r\n",
      "\u001b[1mcollecting ... \u001b[0m\u001b[1m\r",
      "collected 4 items                                                              \u001b[0m\r\n",
      "\r\n",
      "examples/A_helloworld/test_A3_expect_raises.py::test_expect_zerodivisionerror_raised \u001b[32mPASSED\u001b[0m\r\n",
      "examples/A_helloworld/test_A3_expect_raises.py::test_expect_zerodivisionerror_not_raised \u001b[31mFAILED\u001b[0m\r\n",
      "examples/A_helloworld/test_A3_expect_raises.py::test_expect_zerodivisionerror_raised_other \u001b[31mFAILED\u001b[0m\r\n",
      "examples/A_helloworld/test_A3_expect_raises.py::test_expect_typeerror_raised \u001b[32mPASSED\u001b[0m\r\n",
      "\r\n",
      "=================================== FAILURES ===================================\r\n",
      "\u001b[31m\u001b[1m___________________ test_expect_zerodivisionerror_not_raised ___________________\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m    def test_expect_zerodivisionerror_not_raised():  # fails\u001b[0m\r\n",
      "\u001b[1m        with pytest.raises(ZeroDivisionError):\u001b[0m\r\n",
      "\u001b[1m>           2 / 1\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           Failed: DID NOT RAISE <class 'ZeroDivisionError'>\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31mexamples/A_helloworld/test_A3_expect_raises.py\u001b[0m:14: Failed\r\n",
      "\u001b[31m\u001b[1m__________________ test_expect_zerodivisionerror_raised_other __________________\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m    def test_expect_zerodivisionerror_raised_other():  # fails\u001b[0m\r\n",
      "\u001b[1m        with pytest.raises(ZeroDivisionError):\u001b[0m\r\n",
      "\u001b[1m>           2 / \"not a number\"\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           TypeError: unsupported operand type(s) for /: 'int' and 'str'\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31mexamples/A_helloworld/test_A3_expect_raises.py\u001b[0m:19: TypeError\r\n",
      "\u001b[31m========================= \u001b[31m\u001b[1m2 failed\u001b[0m, \u001b[32m2 passed\u001b[0m\u001b[31m in 0.02s\u001b[0m\u001b[31m ==========================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pytest -sv examples/A_helloworld/test_A3_expect_raises.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## before/after\n",
    "\n",
    "- Funciones que se pueden ejecutar antes o después de cada test individual o grupos de tests (módulo/archivo .py)\n",
    "    - `def setup_module(module)` y `def teardown_module(module)` se ejecutan antes del primer test y después del último test del módulo, respectivamente\n",
    "    - `def setup_function(function)` y `def teardown_function(function)` se ejecutan antes y después de cada test individual\n",
    "\n",
    "![Setup/Teardown flow in test functions](images/setup_teardown_flow_functions.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m\"\"\"B1 - Test Before & After at Module level\u001b[39;49;00m\r\n",
      "\u001b[33mFunctions that run before & after all tests on the module, and before & after each test on the module.\u001b[39;49;00m\r\n",
      "\u001b[33mRun with: \"pytest -v -s ...\" to show print output\u001b[39;49;00m\r\n",
      "\u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "my_variable = \u001b[34mNone\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32msetup_module\u001b[39;49;00m():\r\n",
      "    \u001b[34mglobal\u001b[39;49;00m my_variable\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mA) Setup module. Should run BEFORE any test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    my_variable = \u001b[34mTrue\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mteardown_module\u001b[39;49;00m():\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mD) Teardown module. Should run AFTER all tests\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32msetup_function\u001b[39;49;00m():\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mB) Setup function. Should run BEFORE each test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mteardown_function\u001b[39;49;00m():\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mC) Teardown function. Should run AFTER each test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest_my_variable_is_not_none\u001b[39;49;00m():\r\n",
      "    \u001b[34massert\u001b[39;49;00m my_variable \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest_sum\u001b[39;49;00m():\r\n",
      "    \u001b[34massert\u001b[39;49;00m \u001b[34m2\u001b[39;49;00m + \u001b[34m2\u001b[39;49;00m == \u001b[34m4\u001b[39;49;00m\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize examples/B_before_after/test_B1_before_after_module.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform linux -- Python 3.7.4, pytest-5.3.5, py-1.8.1, pluggy-0.13.1 -- /home/david/.miniconda3/bin/python\r\n",
      "cachedir: .pytest_cache\r\n",
      "rootdir: /home/david/Nextcloud/Codigos/Python/2Presentaciones/pyTest\r\n",
      "\u001b[1mcollecting ... \u001b[0m\u001b[1m\r",
      "collected 2 items                                                              \u001b[0m\r\n",
      "\r\n",
      "examples/B_before_after/test_B1_before_after_module.py::test_my_variable_is_not_none A) Setup module. Should run BEFORE any test\r\n",
      "B) Setup function. Should run BEFORE each test\r\n",
      "\u001b[32mPASSED\u001b[0mC) Teardown function. Should run AFTER each test\r\n",
      "\r\n",
      "examples/B_before_after/test_B1_before_after_module.py::test_sum B) Setup function. Should run BEFORE each test\r\n",
      "\u001b[32mPASSED\u001b[0mC) Teardown function. Should run AFTER each test\r\n",
      "D) Teardown module. Should run AFTER all tests\r\n",
      "\r\n",
      "\r\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.00s\u001b[0m\u001b[32m ===============================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pytest -sv examples/B_before_after/test_B1_before_after_module.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test classes + before/after\n",
    "\n",
    "- Los tests se pueden agrupar en clases\n",
    "- Cada test es un método de la clase\n",
    "- Las clases pueden tener métodos `@classmethod def setup_class(cls)` y `@classmethod def teardown_class(cls)`, que de forma similar a los setup/teardown_module, se ejecutan antes del primer test y después del último test presente en la clase\n",
    "\n",
    "![Setup/Teardown flow in test class](images/setup_teardown_flow_class.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m\"\"\"B2 - Test Before & After at Class level\u001b[39;49;00m\r\n",
      "\u001b[33mFunctions that run before & after all tests on the class, and before & after each test on the class.\u001b[39;49;00m\r\n",
      "\u001b[33mRun with: \"pytest -v -s ...\" to show print output\u001b[39;49;00m\r\n",
      "\u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32msetup_module\u001b[39;49;00m(module):\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mA) Setup module. Should run BEFORE any test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mteardown_module\u001b[39;49;00m(module):\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mF) Teardown module. Should run AFTER all tests\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[37m# noinspection PyMethodMayBeStatic,PyMethodParameters\u001b[39;49;00m\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mTestBeforeAfterClass\u001b[39;49;00m:\r\n",
      "    my_variable: \u001b[36mstr\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[90m@classmethod\u001b[39;49;00m\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32msetup_class\u001b[39;49;00m(\u001b[36mcls\u001b[39;49;00m):\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mB) Setup module. Should run BEFORE any test of this class\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        \u001b[36mcls\u001b[39;49;00m.my_variable = \u001b[33m\"\u001b[39;49;00m\u001b[33mOK!\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[90m@classmethod\u001b[39;49;00m\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mteardown_class\u001b[39;49;00m(\u001b[36mcls\u001b[39;49;00m):\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mE) Teardown module. Should run AFTER all tests of this class\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32msetup_method\u001b[39;49;00m(method):\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mC) Setup method. Should run BEFORE each test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mteardown_method\u001b[39;49;00m(method):\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mD) Teardown method. Should run AFTER each test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mtest_my_variable_value\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\r\n",
      "        \u001b[34massert\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.my_variable == \u001b[33m\"\u001b[39;49;00m\u001b[33mOK!\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mtest_my_variable_type\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\r\n",
      "        \u001b[34massert\u001b[39;49;00m \u001b[36mtype\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.my_variable) == \u001b[36mstr\u001b[39;49;00m\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize examples/B_before_after/test_B2_before_after_class.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform linux -- Python 3.7.4, pytest-5.3.5, py-1.8.1, pluggy-0.13.1 -- /home/david/.miniconda3/bin/python\r\n",
      "cachedir: .pytest_cache\r\n",
      "rootdir: /home/david/Nextcloud/Codigos/Python/2Presentaciones/pyTest\r\n",
      "\u001b[1mcollecting ... \u001b[0m\u001b[1m\r",
      "collected 2 items                                                              \u001b[0m\r\n",
      "\r\n",
      "examples/B_before_after/test_B2_before_after_class.py::TestBeforeAfterClass::test_my_variable_value A) Setup module. Should run BEFORE any test\r\n",
      "B) Setup module. Should run BEFORE any test of this class\r\n",
      "C) Setup method. Should run BEFORE each test\r\n",
      "\u001b[32mPASSED\u001b[0mD) Teardown method. Should run AFTER each test\r\n",
      "\r\n",
      "examples/B_before_after/test_B2_before_after_class.py::TestBeforeAfterClass::test_my_variable_type C) Setup method. Should run BEFORE each test\r\n",
      "\u001b[32mPASSED\u001b[0mD) Teardown method. Should run AFTER each test\r\n",
      "E) Teardown module. Should run AFTER all tests of this class\r\n",
      "F) Teardown module. Should run AFTER all tests\r\n",
      "\r\n",
      "\r\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pytest -sv examples/B_before_after/test_B2_before_after_class.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C) Fixtures\n",
    "\n",
    "- Funciones que se pueden invocar en tests individuales, usando su nombre como parámetros de cada función test\n",
    "- El return de la función fixture puede utilizarse en el test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m\"\"\"C1 - Test with Fixtures\u001b[39;49;00m\r\n",
      "\u001b[33mFixtures are functions that run before the test where used, and can return something,\u001b[39;49;00m\r\n",
      "\u001b[33minjected on the test as a function parameter.\u001b[39;49;00m\r\n",
      "\u001b[33mhttps://docs.pytest.org/en/latest/fixture.html\u001b[39;49;00m\r\n",
      "\u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mrandom\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpytest\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[90m@pytest\u001b[39;49;00m.fixture\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mrandom_number\u001b[39;49;00m():\r\n",
      "    \u001b[33m\"\"\"This fixture will return a random number\"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mA) Fixture runs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m random.randint(\u001b[34m0\u001b[39;49;00m, \u001b[34m1000\u001b[39;49;00m)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest_random_number_fixture\u001b[39;49;00m(random_number):\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mB) Test starts\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[34massert\u001b[39;49;00m \u001b[36mtype\u001b[39;49;00m(random_number) == \u001b[36mint\u001b[39;49;00m\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mC) Test ends\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize examples/C_fixtures/test_C1_fixtures.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform linux -- Python 3.7.4, pytest-5.3.5, py-1.8.1, pluggy-0.13.1 -- /home/david/.miniconda3/bin/python\r\n",
      "cachedir: .pytest_cache\r\n",
      "rootdir: /home/david/Nextcloud/Codigos/Python/2Presentaciones/pyTest\r\n",
      "\u001b[1mcollecting ... \u001b[0m\u001b[1m\r",
      "collected 1 item                                                               \u001b[0m\r\n",
      "\r\n",
      "examples/C_fixtures/test_C1_fixtures.py::test_random_number_fixture A) Fixture runs\r\n",
      "B) Test starts\r\n",
      "C) Test ends\r\n",
      "\u001b[32mPASSED\u001b[0m\r\n",
      "\r\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pytest -sv examples/C_fixtures/test_C1_fixtures.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### yield\n",
    "\n",
    "- Los fixtures pueden usar `yield` en lugar de return\n",
    "- Esto nos permite ejecutar código después de los tests en los que se use la fixture (como si fuese un teardown), de forma similar a los context managers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m\"\"\"C2 - Test with Yield Fixtures\u001b[39;49;00m\r\n",
      "\u001b[33mYield fixtures use the \"yield\" statement to return something to the test function,\u001b[39;49;00m\r\n",
      "\u001b[33mbut they keep running after the test (similarly to context managers).\u001b[39;49;00m\r\n",
      "\u001b[33mhttps://pytest.readthedocs.io/en/2.9.1/yieldfixture.html\u001b[39;49;00m\r\n",
      "\u001b[33mhttps://docs.pytest.org/en/latest/yieldfixture.html\u001b[39;49;00m\r\n",
      "\u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mrandom\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpytest\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[90m@pytest\u001b[39;49;00m.fixture\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mrandom_number\u001b[39;49;00m():\r\n",
      "    \u001b[33m\"\"\"This fixture will return a random number\"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mA) Fixture starts\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    number = random.randint(\u001b[34m0\u001b[39;49;00m, \u001b[34m1000\u001b[39;49;00m)\r\n",
      "    \u001b[34myield\u001b[39;49;00m number\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mD) Fixture ends\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest_random_number_fixture_yield\u001b[39;49;00m(random_number):\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mB) Test starts\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[34massert\u001b[39;49;00m \u001b[36mtype\u001b[39;49;00m(random_number) == \u001b[36mint\u001b[39;49;00m\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mC) Test ends\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize examples/C_fixtures/test_C2_yield_fixture.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform linux -- Python 3.7.4, pytest-5.3.5, py-1.8.1, pluggy-0.13.1 -- /home/david/.miniconda3/bin/python\r\n",
      "cachedir: .pytest_cache\r\n",
      "rootdir: /home/david/Nextcloud/Codigos/Python/2Presentaciones/pyTest\r\n",
      "\u001b[1mcollecting ... \u001b[0m\u001b[1m\r",
      "collected 1 item                                                               \u001b[0m\r\n",
      "\r\n",
      "examples/C_fixtures/test_C2_yield_fixture.py::test_random_number_fixture_yield A) Fixture starts\r\n",
      "B) Test starts\r\n",
      "C) Test ends\r\n",
      "\u001b[32mPASSED\u001b[0mD) Fixture ends\r\n",
      "\r\n",
      "\r\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.00s\u001b[0m\u001b[32m ===============================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pytest -sv examples/C_fixtures/test_C2_yield_fixture.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tmp_path fixture\n",
    "\n",
    "- Una de las fixtures incluídas por defecto\n",
    "- Nos devuelve un objeto Path apuntando a un directorio temporal en el que podemos escribir y leer ficheros para nuestros tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m\"\"\"C3 - tmp_path built-in fixture\u001b[39;49;00m\r\n",
      "\u001b[33mUsage of the built-in tmp_path fixture, which provides a temp path to save/read files into/from\u001b[39;49;00m\r\n",
      "\u001b[33mhttps://docs.pytest.org/en/latest/tmpdir.html\u001b[39;49;00m\r\n",
      "\u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpathlib\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Path\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest_write_read_file\u001b[39;49;00m(tmp_path: Path):\r\n",
      "    file_path = tmp_path / \u001b[33m\"\u001b[39;49;00m\u001b[33mhello.txt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    file_path = file_path.as_posix()\r\n",
      "    lines = [\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mHello there!\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mGeneral Kenobi!\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    ]\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mThe file path is\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, file_path)\r\n",
      "\r\n",
      "    \u001b[37m# Write\u001b[39;49;00m\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(file_path, \u001b[33m\"\u001b[39;49;00m\u001b[33mw\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m file:\r\n",
      "        file.writelines(\u001b[33m\"\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join(lines))\r\n",
      "\r\n",
      "    \u001b[37m# Read\u001b[39;49;00m\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(file_path, \u001b[33m\"\u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m file:\r\n",
      "        file_content = file.read()\r\n",
      "        \u001b[34mfor\u001b[39;49;00m line \u001b[35min\u001b[39;49;00m lines:\r\n",
      "            \u001b[34massert\u001b[39;49;00m line \u001b[35min\u001b[39;49;00m file_content\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize examples/C_fixtures/test_C3_tmp_path.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform linux -- Python 3.7.4, pytest-5.3.5, py-1.8.1, pluggy-0.13.1 -- /home/david/.miniconda3/bin/python\r\n",
      "cachedir: .pytest_cache\r\n",
      "rootdir: /home/david/Nextcloud/Codigos/Python/2Presentaciones/pyTest\r\n",
      "\u001b[1mcollecting ... \u001b[0m\u001b[1m\r",
      "collected 1 item                                                               \u001b[0m\r\n",
      "\r\n",
      "examples/C_fixtures/test_C3_tmp_path.py::test_write_read_file The file path is /tmp/pytest-of-david/pytest-1/test_write_read_file0/hello.txt\r\n",
      "\u001b[32mPASSED\u001b[0m\r\n",
      "\r\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.00s\u001b[0m\u001b[32m ===============================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pytest -sv examples/C_fixtures/test_C3_tmp_path.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there!\n",
      "General Kenobi!\n"
     ]
    }
   ],
   "source": [
    "# Leemos el fichero que -supuestamente- se generó\n",
    "\n",
    "from getpass import getuser\n",
    "\n",
    "for i in range(10):\n",
    "    try:\n",
    "        FILENAME = f\"/tmp/pytest-of-{getuser()}/pytest-{i}/test_write_read_file0/hello.txt\"\n",
    "\n",
    "        with open(FILENAME, \"r\") as file:\n",
    "            print(file.read())\n",
    "            break\n",
    "    except FileNotFoundError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D) Herencia de clases de tests\n",
    "\n",
    "- Agrupar tests en clases nos permite aprovechar las ventajas de la herencia de clases\n",
    "- Por ejemplo, crear métodos setup/teardown en una clase \"BaseTest\" que sean usados (o expandibles) por otras clases de tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[33m\"\"\"D - BASE TEST\u001b[39;49;00m\n\u001b[33mDefine a class with setup & teardown methods that will be inherited from test classes\u001b[39;49;00m\n\u001b[33m\"\"\"\u001b[39;49;00m\n\n\n\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mBaseTest\u001b[39;49;00m:\n    \u001b[90m@classmethod\u001b[39;49;00m\n    \u001b[34mdef\u001b[39;49;00m \u001b[32msetup_class\u001b[39;49;00m(\u001b[36mcls\u001b[39;49;00m):\n        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mA) Setup class\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n\n    \u001b[90m@classmethod\u001b[39;49;00m\n    \u001b[34mdef\u001b[39;49;00m \u001b[32mteardown_class\u001b[39;49;00m(\u001b[36mcls\u001b[39;49;00m):\n        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mE) Teardown class\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n\n    \u001b[34mdef\u001b[39;49;00m \u001b[32msetup_method\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\n        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mB) Setup method\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n\n    \u001b[34mdef\u001b[39;49;00m \u001b[32mteardown_method\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\n        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mD) Teardown class\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize examples/D_class_inherit/base_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[33m\"\"\"D1 - Test class-defined test inheriting from BaseTest\u001b[39;49;00m\n\u001b[33m\"\"\"\u001b[39;49;00m\n\n\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mbase_test\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m BaseTest\n\n\n\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mTestMain\u001b[39;49;00m(BaseTest):\n    \u001b[34mdef\u001b[39;49;00m \u001b[32mtest_class_1\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\n        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mC) Inside test class method\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n\n    \u001b[34mdef\u001b[39;49;00m \u001b[32mtest_class_2\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\n        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mC) Inside test class method\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize examples/D_class_inherit/test_D1_class_inherit.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\nplatform linux -- Python 3.7.4, pytest-5.3.5, py-1.8.1, pluggy-0.13.1 -- /home/david/.miniconda3/bin/python\ncachedir: .pytest_cache\nrootdir: /home/david/Nextcloud/Codigos/Python/2Presentaciones/pyTest\ncollected 2 items                                                              \u001b[0m\n\nexamples/D_class_inherit/test_D1_class_inherit.py::TestMain::test_class_1 A) Setup class\nB) Setup method\nC) Inside test class method\n\u001b[32mPASSED\u001b[0mD) Teardown class\n\nexamples/D_class_inherit/test_D1_class_inherit.py::TestMain::test_class_2 B) Setup method\nC) Inside test class method\n\u001b[32mPASSED\u001b[0mD) Teardown class\nE) Teardown class\n\n\n\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -sv examples/D_class_inherit/test_D1_class_inherit.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E) Mocking\n",
    "\n",
    "- En ocasiones, queremos hacer tests evitando que se llame a ciertas funciones o métodos. Por ejemplo, evitando que se haga una request HTTP o que se lea/escriba en una base de datos.\n",
    "- Para ello hacemos monkeypatching de las funciones que realizan estas tareas, sustituyéndolas por otras funciones creadas específicamente para nuestros tests. A menudo devolverán resultados ficticios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E1.- monkeypatching requests.get\n",
    "\n",
    "- Tenemos una función `get_ip()` en el módulo `ipify_client.py` que consulta la api de Ipify, y nos devuelve nuestra IP pública\n",
    "- Queremos testear la función sin que se haga la request a la API real\n",
    "- Haremos que, cuando se llame al método `requests.get(...)`, se nos devuelva una Response personalizada (mocked Response)\n",
    "- Para ello creamos una función que reemplazará al método `requests.get(...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m\"\"\"ipify client\u001b[39;49;00m\r\n",
      "\u001b[33mRequest the ipify API (https://www.ipify.org/), that returns our current public IP, using requests.get()\u001b[39;49;00m\r\n",
      "\u001b[33mUsed by tests E1, E2\u001b[39;49;00m\r\n",
      "\u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mrequests\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mget_ip\u001b[39;49;00m() -> \u001b[36mstr\u001b[39;49;00m:\r\n",
      "    response = requests.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mhttps://api.ipify.org/?format=raw\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    response.raise_for_status()\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m response.text.strip()\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize examples/E_mocking/ipify_client.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m\"\"\"E1 - Test monkeypatching requests.get() method manually\u001b[39;49;00m\n",
      "\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpytest\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrequests\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Response, HTTPError\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mipify_client\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m get_ip\n",
      "\n",
      "MOCKED_IP = \u001b[33m\"\u001b[39;49;00m\u001b[33m0.0.0.0\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\u001b[33m\"\"\"The value the mocked API will return\"\"\"\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mMockedResponse\u001b[39;49;00m(Response):\n",
      "    \u001b[33m\"\"\"The requests.Response class does not allow setting the attribute \"text\",\u001b[39;49;00m\n",
      "\u001b[33m    so we create a custom inheriting class that returns the attribute \"_text\" when reading \"text\",\u001b[39;49;00m\n",
      "\u001b[33m    by overriding the __getattribute__ magic method.\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, text, status_code=\u001b[34m200\u001b[39;49;00m):\n",
      "        \u001b[36msuper\u001b[39;49;00m().\u001b[32m__init__\u001b[39;49;00m()\n",
      "        \u001b[36mself\u001b[39;49;00m._text = text\n",
      "        \u001b[36mself\u001b[39;49;00m.status_code = status_code\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__getattribute__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, item):\n",
      "        \u001b[34mif\u001b[39;49;00m item == \u001b[33m\"\u001b[39;49;00m\u001b[33mtext\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "            \u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m._text\n",
      "        \u001b[34melse\u001b[39;49;00m:\n",
      "            \u001b[34mreturn\u001b[39;49;00m \u001b[36msuper\u001b[39;49;00m().\u001b[32m__getattribute__\u001b[39;49;00m(item)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_mocked_get_200\u001b[39;49;00m(*args, **kwargs) -> Response:\n",
      "    \u001b[33m\"\"\"We will monkeypatch the get() method from the installed requests library.\u001b[39;49;00m\n",
      "\u001b[33m    ipify_client.get_ip() will use this mocked function instead of the original method.\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m MockedResponse(text=MOCKED_IP, status_code=\u001b[34m200\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_mocked_get_404\u001b[39;49;00m(*args, **kwargs) -> Response:\n",
      "    \u001b[33m\"\"\"This function returns 404, thus ipify_client.get_ip() will return RequestError\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m MockedResponse(text=MOCKED_IP, status_code=\u001b[34m404\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest_mocked_get_200\u001b[39;49;00m(monkeypatch):\n",
      "    monkeypatch.setattr(\u001b[33m\"\u001b[39;49;00m\u001b[33mrequests.get\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _mocked_get_200)\n",
      "\n",
      "    \u001b[34massert\u001b[39;49;00m get_ip() == MOCKED_IP\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest_mocked_get_404\u001b[39;49;00m(monkeypatch):\n",
      "    monkeypatch.setattr(\u001b[33m\"\u001b[39;49;00m\u001b[33mrequests.get\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _mocked_get_404)\n",
      "\n",
      "    \u001b[34mwith\u001b[39;49;00m pytest.raises(HTTPError) \u001b[34mas\u001b[39;49;00m error:\n",
      "        get_ip()\n",
      "\n",
      "    \u001b[34massert\u001b[39;49;00m error.value.response.status_code == \u001b[34m404\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "!pygmentize examples/E_mocking/test_E1_monkeypatch_requests_get.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.1, pytest-5.3.5, py-1.8.1, pluggy-0.13.1 -- /home/david/.miniconda3/envs/pytest/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/david/Nextcloud/Codigos/Python/2Presentaciones/pyTest\n",
      "plugins: xdist-1.31.0, asyncio-0.10.0, forked-1.1.3, mock-2.0.0\n",
      "collected 2 items                                                              \u001b[0m\n",
      "\n",
      "examples/E_mocking/test_E1_monkeypatch_requests_get.py::test_mocked_get_200 \u001b[32mPASSED\u001b[0m\n",
      "examples/E_mocking/test_E1_monkeypatch_requests_get.py::test_mocked_get_404 \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.09s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -sv examples/E_mocking/test_E1_monkeypatch_requests_get.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E2.- librería `responses` para mockear requests\n",
    "\n",
    "- la librería `responses` nos facilita este trabajo con métodos para mockear respuestas a requests concretas realizadas contra una URL concreta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m\"\"\"E2 - Test using responses library to monkeypatch HTTP calls\u001b[39;49;00m\r\n",
      "\u001b[33mhttps://github.com/getsentry/responses\u001b[39;49;00m\r\n",
      "\u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpytest\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mresponses\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrequests\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m HTTPError\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mipify_client\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m get_ip\r\n",
      "\r\n",
      "MOCKED_IP = \u001b[33m\"\u001b[39;49;00m\u001b[33m0.0.0.0\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\u001b[33m\"\"\"The value the mocked API will return\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[90m@responses\u001b[39;49;00m.activate\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest_mocked_http_call_with_responses_200\u001b[39;49;00m():\r\n",
      "    responses.add(\r\n",
      "        method=responses.GET,\r\n",
      "        url=\u001b[33m\"\u001b[39;49;00m\u001b[33mhttps://api.ipify.org/?format=raw\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        body=MOCKED_IP,\r\n",
      "        status=\u001b[34m200\u001b[39;49;00m\r\n",
      "    )\r\n",
      "\r\n",
      "    \u001b[34massert\u001b[39;49;00m get_ip() == MOCKED_IP\r\n",
      "\r\n",
      "    \u001b[37m# Verify how many times requests.get was called, and with which params\u001b[39;49;00m\r\n",
      "    \u001b[34massert\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(responses.calls) == \u001b[34m1\u001b[39;49;00m\r\n",
      "    \u001b[34massert\u001b[39;49;00m responses.calls[\u001b[34m0\u001b[39;49;00m].request.url == \u001b[33m\"\u001b[39;49;00m\u001b[33mhttps://api.ipify.org/?format=raw\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[90m@responses\u001b[39;49;00m.activate\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest_mocked_http_call_with_responses_404\u001b[39;49;00m():\r\n",
      "    responses.add(\r\n",
      "        method=responses.GET,\r\n",
      "        url=\u001b[33m\"\u001b[39;49;00m\u001b[33mhttps://api.ipify.org/?format=raw\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        body=\u001b[33m\"\u001b[39;49;00m\u001b[33mKO\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        status=\u001b[34m404\u001b[39;49;00m\r\n",
      "    )\r\n",
      "\r\n",
      "    \u001b[34mwith\u001b[39;49;00m pytest.raises(HTTPError) \u001b[34mas\u001b[39;49;00m error:\r\n",
      "        get_ip()\r\n",
      "\r\n",
      "    \u001b[34massert\u001b[39;49;00m error.value.response.status_code == \u001b[34m404\u001b[39;49;00m\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize examples/E_mocking/test_E2_monkeypatch_responses.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.1, pytest-5.3.5, py-1.8.1, pluggy-0.13.1 -- /home/david/.miniconda3/envs/pytest/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/david/Nextcloud/Codigos/Python/2Presentaciones/pyTest\n",
      "plugins: xdist-1.31.0, asyncio-0.10.0, forked-1.1.3, mock-2.0.0\n",
      "collected 2 items                                                              \u001b[0m\n",
      "\n",
      "examples/E_mocking/test_E2_monkeypatch_responses.py::test_mocked_http_call_with_responses_200 \u001b[32mPASSED\u001b[0m\n",
      "examples/E_mocking/test_E2_monkeypatch_responses.py::test_mocked_http_call_with_responses_404 \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.09s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -sv examples/E_mocking/test_E2_monkeypatch_responses.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E3.- mocking with `pytest-mock`\n",
    "\n",
    "- Usaremos la fixture `mocker` de la librería `pytest-mock` para mockear la función `divide` del módulo `calculator_client.py`\n",
    "- `mocker.patch` nos permite hacer monkeypatching a la función para que devuelva/ejecute lo que queramos durante el test\n",
    "- `mocker.spy` nos permite analizar con qué parámetros y cuántas veces se ha llamado a la función/método"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m\"\"\"calculator client\u001b[39;49;00m\r\n",
      "\u001b[33mUsed by test E3\u001b[39;49;00m\r\n",
      "\u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mdivide\u001b[39;49;00m(a, b):\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m a / b\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize examples/E_mocking/calculator_client.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m\"\"\"E3 - Test mocking functions with pytest-mock \"mocker\" fixture.\u001b[39;49;00m\r\n",
      "\u001b[33mhttps://pypi.org/project/pytest-mock/\u001b[39;49;00m\r\n",
      "\u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m.\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m calculator_client\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest_monkeypatch\u001b[39;49;00m(mocker):\r\n",
      "    \u001b[33m\"\"\"mocker fixture includes the same methods as mock.patch from the unittest built-in lib\u001b[39;49;00m\r\n",
      "\u001b[33m    (https://docs.python.org/3/library/unittest.mock.html#patch).\u001b[39;49;00m\r\n",
      "\u001b[33m    Using it as a fixture avoids having to use as context managers or decorators\u001b[39;49;00m\r\n",
      "\u001b[33m    (https://github.com/pytest-dev/pytest-mock/#why-bother-with-a-plugin).\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m_mocked_divide\u001b[39;49;00m(a, b):\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[34m0\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[37m# the absolute module path is required, although doing relative import\u001b[39;49;00m\r\n",
      "    mocker.patch(\u001b[33m\"\u001b[39;49;00m\u001b[33mE_mocking.calculator_client.divide\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _mocked_divide)\r\n",
      "\r\n",
      "    \u001b[34massert\u001b[39;49;00m calculator_client.divide(\u001b[34m10\u001b[39;49;00m, \u001b[34m0\u001b[39;49;00m) == \u001b[34m0\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest_spy\u001b[39;49;00m(mocker):\r\n",
      "    \u001b[33m\"\"\"Mocking with the spy method can assert if a mocked method/function was called,\u001b[39;49;00m\r\n",
      "\u001b[33m    with which parameters and what returns\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "    spy_calculator = mocker.spy(calculator_client, \u001b[33m\"\u001b[39;49;00m\u001b[33mdivide\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[34massert\u001b[39;49;00m calculator_client.divide(\u001b[34m10\u001b[39;49;00m, \u001b[34m5\u001b[39;49;00m) == \u001b[34m2\u001b[39;49;00m\r\n",
      "\r\n",
      "    spy_calculator.assert_called_once_with(\u001b[34m10\u001b[39;49;00m, \u001b[34m5\u001b[39;49;00m)\r\n",
      "    \u001b[34massert\u001b[39;49;00m spy_calculator.spy_return == \u001b[34m2\u001b[39;49;00m\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize examples/E_mocking/test_E3_pytest_mock.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.1, pytest-5.3.5, py-1.8.1, pluggy-0.13.1 -- /home/david/.miniconda3/envs/pytest/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/david/Nextcloud/Codigos/Python/2Presentaciones/pyTest\n",
      "plugins: xdist-1.31.0, asyncio-0.10.0, forked-1.1.3, mock-2.0.0\n",
      "collected 2 items                                                              \u001b[0m\n",
      "\n",
      "examples/E_mocking/test_E3_pytest_mock.py::test_monkeypatch \u001b[32mPASSED\u001b[0m\n",
      "examples/E_mocking/test_E3_pytest_mock.py::test_spy \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.06s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -sv examples/E_mocking/test_E3_pytest_mock.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F) Parametrize\n",
    "\n",
    "- Teniendo un test con argumentos, podemos ejecutar el test varias veces con varios conjuntos de argumentos\n",
    "- En el siguiente ejemplo, `test_sum` tiene 3 argumentos: los 2 números a sumar y el resultado esperado\n",
    "- Con `pytest.mark.parametrize` indicamos con qué argumentos se ejecutará el test\n",
    "- Con `pytest.param(..., marks=pytest.mark.xfail)` podemos indicar que el test con el conjunto de argumentos dado debería fallar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m\"\"\"F1 - Test function that will run with a set of defined parameters\u001b[39;49;00m\r\n",
      "\u001b[33mhttps://docs.pytest.org/en/latest/parametrize.html#parametrize\u001b[39;49;00m\r\n",
      "\u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpytest\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[90m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33ma,b,expected\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\r\n",
      "    (\u001b[34m2\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m, \u001b[34m4\u001b[39;49;00m),\r\n",
      "    (\u001b[34m10\u001b[39;49;00m, \u001b[34m10\u001b[39;49;00m, \u001b[34m20\u001b[39;49;00m),\r\n",
      "    (-\u001b[34m5\u001b[39;49;00m, \u001b[34m5\u001b[39;49;00m, \u001b[34m0\u001b[39;49;00m),\r\n",
      "    (\u001b[34m0\u001b[39;49;00m, \u001b[34m0\u001b[39;49;00m, \u001b[34m0\u001b[39;49;00m)\r\n",
      "])\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest_sum\u001b[39;49;00m(a, b, expected):\r\n",
      "    result = a + b\r\n",
      "    \u001b[34massert\u001b[39;49;00m result == expected\r\n",
      "\r\n",
      "\r\n",
      "\u001b[90m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33ma,b,expected\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\r\n",
      "    (\u001b[34m2\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m, \u001b[34m4\u001b[39;49;00m),\r\n",
      "    (\u001b[34m5\u001b[39;49;00m, \u001b[34m5\u001b[39;49;00m, \u001b[34m10\u001b[39;49;00m),\r\n",
      "    pytest.param(\u001b[34m1\u001b[39;49;00m, \u001b[34m1\u001b[39;49;00m, \u001b[34m7\u001b[39;49;00m, marks=pytest.mark.xfail)  \u001b[37m# expect 1+1=7 to fail\u001b[39;49;00m\r\n",
      "])\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest_sum_expect_failing\u001b[39;49;00m(a, b, expected):\r\n",
      "    \u001b[33m\"\"\"The last given set of parameters will fail, and we expect it to fail\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "    result = a + b\r\n",
      "    \u001b[34massert\u001b[39;49;00m result == expected\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize examples/F_parametrize/test_F1_parametrize.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.1, pytest-5.3.5, py-1.8.1, pluggy-0.13.1 -- /home/david/.miniconda3/envs/pytest/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/david/Nextcloud/Codigos/Python/2Presentaciones/pyTest\n",
      "plugins: xdist-1.31.0, asyncio-0.10.0, forked-1.1.3, mock-2.0.0\n",
      "collected 7 items                                                              \u001b[0m\n",
      "\n",
      "examples/F_parametrize/test_F1_parametrize.py::test_sum[2-2-4] \u001b[32mPASSED\u001b[0m\n",
      "examples/F_parametrize/test_F1_parametrize.py::test_sum[10-10-20] \u001b[32mPASSED\u001b[0m\n",
      "examples/F_parametrize/test_F1_parametrize.py::test_sum[-5-5-0] \u001b[32mPASSED\u001b[0m\n",
      "examples/F_parametrize/test_F1_parametrize.py::test_sum[0-0-0] \u001b[32mPASSED\u001b[0m\n",
      "examples/F_parametrize/test_F1_parametrize.py::test_sum_expect_failing[2-2-4] \u001b[32mPASSED\u001b[0m\n",
      "examples/F_parametrize/test_F1_parametrize.py::test_sum_expect_failing[5-5-10] \u001b[32mPASSED\u001b[0m\n",
      "examples/F_parametrize/test_F1_parametrize.py::test_sum_expect_failing[1-1-7] \u001b[33mXFAIL\u001b[0m\n",
      "\n",
      "\u001b[32m========================= \u001b[32m\u001b[1m6 passed\u001b[0m, \u001b[33m1 xfailed\u001b[0m\u001b[32m in 0.07s\u001b[0m\u001b[32m =========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -sv examples/F_parametrize/test_F1_parametrize.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F2.- test con conjuntos de argumentos que fallan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m\"\"\"F2 - Test function that will run with a set of defined parameters,\u001b[39;49;00m\r\n",
      "\u001b[33mbut some of the given parameters will fail.\u001b[39;49;00m\r\n",
      "\u001b[33mhttps://docs.pytest.org/en/latest/parametrize.html#parametrize\u001b[39;49;00m\r\n",
      "\u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpytest\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[90m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33ma,b,expected\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\r\n",
      "    (\u001b[34m2\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m, \u001b[34m4\u001b[39;49;00m),\r\n",
      "    (\u001b[34m1\u001b[39;49;00m, \u001b[34m1\u001b[39;49;00m, \u001b[34m7\u001b[39;49;00m),\r\n",
      "    (\u001b[34m20\u001b[39;49;00m, \u001b[34m1\u001b[39;49;00m, \u001b[34m21\u001b[39;49;00m),\r\n",
      "    (-\u001b[34m5\u001b[39;49;00m, -\u001b[34m5\u001b[39;49;00m, \u001b[34m10\u001b[39;49;00m)\r\n",
      "])\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest_sum\u001b[39;49;00m(a, b, expected):\r\n",
      "    result = a + b\r\n",
      "    \u001b[34massert\u001b[39;49;00m result == expected\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize examples/F_parametrize/test_F2_parametrize_failing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.1, pytest-5.3.5, py-1.8.1, pluggy-0.13.1 -- /home/david/.miniconda3/envs/pytest/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/david/Nextcloud/Codigos/Python/2Presentaciones/pyTest\n",
      "plugins: xdist-1.31.0, asyncio-0.10.0, forked-1.1.3, mock-2.0.0\n",
      "collected 4 items                                                              \u001b[0m\n",
      "\n",
      "examples/F_parametrize/test_F2_parametrize_failing.py::test_sum[2-2-4] \u001b[32mPASSED\u001b[0m\n",
      "examples/F_parametrize/test_F2_parametrize_failing.py::test_sum[1-1-7] \u001b[31mFAILED\u001b[0m\n",
      "examples/F_parametrize/test_F2_parametrize_failing.py::test_sum[20-1-21] \u001b[32mPASSED\u001b[0m\n",
      "examples/F_parametrize/test_F2_parametrize_failing.py::test_sum[-5--5-10] \u001b[31mFAILED\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_______________________________ test_sum[1-1-7] ________________________________\u001b[0m\n",
      "\n",
      "a = 1, b = 1, expected = 7\n",
      "\n",
      "\u001b[1m    @pytest.mark.parametrize(\"a,b,expected\", [\u001b[0m\n",
      "\u001b[1m        (2, 2, 4),\u001b[0m\n",
      "\u001b[1m        (1, 1, 7),\u001b[0m\n",
      "\u001b[1m        (20, 1, 21),\u001b[0m\n",
      "\u001b[1m        (-5, -5, 10)\u001b[0m\n",
      "\u001b[1m    ])\u001b[0m\n",
      "\u001b[1m    def test_sum(a, b, expected):\u001b[0m\n",
      "\u001b[1m        result = a + b\u001b[0m\n",
      "\u001b[1m>       assert result == expected\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 2 == 7\u001b[0m\n",
      "\u001b[1m\u001b[31mE         -2\u001b[0m\n",
      "\u001b[1m\u001b[31mE         +7\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mexamples/F_parametrize/test_F2_parametrize_failing.py\u001b[0m:17: AssertionError\n",
      "\u001b[31m\u001b[1m______________________________ test_sum[-5--5-10] ______________________________\u001b[0m\n",
      "\n",
      "a = -5, b = -5, expected = 10\n",
      "\n",
      "\u001b[1m    @pytest.mark.parametrize(\"a,b,expected\", [\u001b[0m\n",
      "\u001b[1m        (2, 2, 4),\u001b[0m\n",
      "\u001b[1m        (1, 1, 7),\u001b[0m\n",
      "\u001b[1m        (20, 1, 21),\u001b[0m\n",
      "\u001b[1m        (-5, -5, 10)\u001b[0m\n",
      "\u001b[1m    ])\u001b[0m\n",
      "\u001b[1m    def test_sum(a, b, expected):\u001b[0m\n",
      "\u001b[1m        result = a + b\u001b[0m\n",
      "\u001b[1m>       assert result == expected\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert -10 == 10\u001b[0m\n",
      "\u001b[1m\u001b[31mE         --10\u001b[0m\n",
      "\u001b[1m\u001b[31mE         +10\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mexamples/F_parametrize/test_F2_parametrize_failing.py\u001b[0m:17: AssertionError\n",
      "\u001b[31m========================= \u001b[31m\u001b[1m2 failed\u001b[0m, \u001b[32m2 passed\u001b[0m\u001b[31m in 0.06s\u001b[0m\u001b[31m ==========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -sv examples/F_parametrize/test_F2_parametrize_failing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F3.- matrices de conjuntos\n",
    "\n",
    "- Un test puede tener todos los decoradores `pytest.mark.parametrize(...)` que queramos\n",
    "- Se ejecutará el test con todas las variaciones posibles\n",
    "- En el siguiente ejemplo, con 2 argumentos, 2 decoradores `parametrize` definiendo las posibilidades de cada argumento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m\"\"\"F3 - Test stacking multiple pytest.mark.parametrize decorators to create a matrix of parameters\u001b[39;49;00m\r\n",
      "\u001b[33mhttps://docs.pytest.org/en/latest/parametrize.html#parametrize\u001b[39;49;00m\r\n",
      "\u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpytest\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[90m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mstr1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\r\n",
      "    \u001b[33m\"\u001b[39;49;00m\u001b[33mThat\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33ms impossible\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "    \u001b[33m\"\u001b[39;49;00m\u001b[33mIronic\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "    \u001b[33m\"\u001b[39;49;00m\u001b[33mAre you threating me?\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "])\r\n",
      "\u001b[90m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mstr2\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\r\n",
      "    \u001b[33m\"\u001b[39;49;00m\u001b[33mPerhaps the archives are incomplete\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "    \u001b[33m\"\u001b[39;49;00m\u001b[33mI am the senate!\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "])\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest_join_strings\u001b[39;49;00m(str1, str2):\r\n",
      "    \u001b[33m\"\"\"6 tests will run (parameters of decorator 1 * parameters of decorator 2)\"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[34massert\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join([str1, str2]) == \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{str1}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m{str2}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize examples/F_parametrize/test_F3_matrix_parametrize.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform linux -- Python 3.8.1, pytest-5.3.5, py-1.8.1, pluggy-0.13.1 -- /home/david/.miniconda3/envs/pytest/bin/python\r\n",
      "cachedir: .pytest_cache\r\n",
      "rootdir: /home/david/Nextcloud/Codigos/Python/2Presentaciones/pyTest\r\n",
      "plugins: xdist-1.31.0, asyncio-0.10.0, forked-1.1.3, mock-2.0.0\r\n",
      "\u001b[1mcollecting ... \u001b[0m\u001b[1m\r",
      "collected 6 items                                                              \u001b[0m\r\n",
      "\r\n",
      "examples/F_parametrize/test_F3_matrix_parametrize.py::test_join_strings[Perhaps the archives are incomplete-That's impossible] \u001b[32mPASSED\u001b[0m\r\n",
      "examples/F_parametrize/test_F3_matrix_parametrize.py::test_join_strings[Perhaps the archives are incomplete-Ironic] \u001b[32mPASSED\u001b[0m\r\n",
      "examples/F_parametrize/test_F3_matrix_parametrize.py::test_join_strings[Perhaps the archives are incomplete-Are you threating me?] \u001b[32mPASSED\u001b[0m\r\n",
      "examples/F_parametrize/test_F3_matrix_parametrize.py::test_join_strings[I am the senate!-That's impossible] \u001b[32mPASSED\u001b[0m\r\n",
      "examples/F_parametrize/test_F3_matrix_parametrize.py::test_join_strings[I am the senate!-Ironic] \u001b[32mPASSED\u001b[0m\r\n",
      "examples/F_parametrize/test_F3_matrix_parametrize.py::test_join_strings[I am the senate!-Are you threating me?] \u001b[32mPASSED\u001b[0m\r\n",
      "\r\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m6 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[32m ===============================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pytest -sv examples/F_parametrize/test_F3_matrix_parametrize.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F4.- parametrizando `pytest.raises`\n",
    "\n",
    "- Queremos parametrizar que algunos argumentos harán pasar el test, y otros harán que se lance una excepción concreta (lo cual comprobamos con el context manager `with pytest.raises(...)`\n",
    "- Para ello, uno de los argumentos es definir el context manager con la excepción que se lanzará\n",
    "- En los tests que se ejecutarán sin lanzar excepción, definimos un context manager vacío"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m\"\"\"F4 - Test parametrizing context managers to parametrize pytest.raises\u001b[39;49;00m\r\n",
      "\u001b[33mhttps://docs.pytest.org/en/latest/example/parametrize.html#parametrizing-conditional-raising\u001b[39;49;00m\r\n",
      "\u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcontextlib\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpytest\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[90m@contextlib\u001b[39;49;00m.contextmanager\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mdoes_not_raise\u001b[39;49;00m():\r\n",
      "    \u001b[33m\"\"\"empty (noop) context manager\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[34myield\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[90m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mx,y,expected_context_manager,expected_result\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\r\n",
      "    (\u001b[34m4\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m, does_not_raise(), \u001b[34m2\u001b[39;49;00m),\r\n",
      "    (\u001b[34m10\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m, does_not_raise(), \u001b[34m5\u001b[39;49;00m),\r\n",
      "    (\u001b[34m5\u001b[39;49;00m, \u001b[34m0\u001b[39;49;00m, pytest.raises(\u001b[36mZeroDivisionError\u001b[39;49;00m), \u001b[34mNone\u001b[39;49;00m),\r\n",
      "    (\u001b[33m\"\u001b[39;49;00m\u001b[33mnot a number\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[34m5\u001b[39;49;00m, pytest.raises(\u001b[36mTypeError\u001b[39;49;00m), \u001b[34mNone\u001b[39;49;00m)\r\n",
      "])\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest_divide\u001b[39;49;00m(x, y, expected_context_manager, expected_result):\r\n",
      "    \u001b[34mwith\u001b[39;49;00m expected_context_manager:\r\n",
      "        result = x / y\r\n",
      "        \u001b[34massert\u001b[39;49;00m result == expected_result\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize examples/F_parametrize/test_F4_parametrize_raises.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform linux -- Python 3.8.1, pytest-5.3.5, py-1.8.1, pluggy-0.13.1 -- /home/david/.miniconda3/envs/pytest/bin/python\r\n",
      "cachedir: .pytest_cache\r\n",
      "rootdir: /home/david/Nextcloud/Codigos/Python/2Presentaciones/pyTest\r\n",
      "plugins: xdist-1.31.0, asyncio-0.10.0, forked-1.1.3, mock-2.0.0\r\n",
      "\u001b[1mcollecting ... \u001b[0m\u001b[1m\r",
      "collected 4 items                                                              \u001b[0m\r\n",
      "\r\n",
      "examples/F_parametrize/test_F4_parametrize_raises.py::test_divide[4-2-expected_context_manager0-2] \u001b[32mPASSED\u001b[0m\r\n",
      "examples/F_parametrize/test_F4_parametrize_raises.py::test_divide[10-2-expected_context_manager1-5] \u001b[32mPASSED\u001b[0m\r\n",
      "examples/F_parametrize/test_F4_parametrize_raises.py::test_divide[5-0-expected_context_manager2-None] \u001b[32mPASSED\u001b[0m\r\n",
      "examples/F_parametrize/test_F4_parametrize_raises.py::test_divide[not a number-5-expected_context_manager3-None] \u001b[32mPASSED\u001b[0m\r\n",
      "\r\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m4 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[32m ===============================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pytest -sv examples/F_parametrize/test_F4_parametrize_raises.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G) async tests\n",
    "\n",
    "- Para ejecutar tests con coroutines (funciones/métodos async), necesitamos un plugin async, como `pytest-asyncio`\n",
    "- Este plugin nos permite crear funciones test asíncronas, e incluye una nueva fixture para obtener el event loop desde el que ejecutar coroutines\n",
    "- En el siguiente ejemplo, queremos testear la función asíncrona `get_async()` del módulo `async_tools.py`, que hace una request asíncrona a la API de reqres.in con la librería httpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m\"\"\"async tools\u001b[39;49;00m\r\n",
      "\u001b[33mPerform HTTP requests to the API in https://reqres.in/, using httpx.AsyncClient.\u001b[39;49;00m\r\n",
      "\u001b[33mUsed by test G1\u001b[39;49;00m\r\n",
      "\u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mrandom\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mhttpx\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34masync\u001b[39;49;00m \u001b[34mdef\u001b[39;49;00m \u001b[32mget_async\u001b[39;49;00m(sleep_time=\u001b[34mNone\u001b[39;49;00m):\r\n",
      "    \u001b[34mif\u001b[39;49;00m sleep_time \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\r\n",
      "        sleep_time = random.randint(\u001b[34m0\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[34masync\u001b[39;49;00m \u001b[34mwith\u001b[39;49;00m httpx.AsyncClient() \u001b[34mas\u001b[39;49;00m client:\r\n",
      "        response = \u001b[34mawait\u001b[39;49;00m client.get(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mhttps://reqres.in/api/users?delay=\u001b[39;49;00m\u001b[33m{sleep_time}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        response.raise_for_status()\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m response\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize examples/G_asyncio/async_tools.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m\"\"\"G1 - testing async code (calling async functions/methods)\u001b[39;49;00m\r\n",
      "\u001b[33mhttps://github.com/pytest-dev/pytest-asyncio\u001b[39;49;00m\r\n",
      "\u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpytest\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36masyncio\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36masync_tools\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m get_async\r\n",
      "\r\n",
      "\r\n",
      "\u001b[90m@pytest\u001b[39;49;00m.mark.asyncio\r\n",
      "\u001b[34masync\u001b[39;49;00m \u001b[34mdef\u001b[39;49;00m \u001b[32mtest_async_code\u001b[39;49;00m():\r\n",
      "    \u001b[33m\"\"\"Having an async test function (coroutine)\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "    response = \u001b[34mawait\u001b[39;49;00m get_async()\r\n",
      "    \u001b[34massert\u001b[39;49;00m response.status_code == \u001b[34m200\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest_event_loop\u001b[39;49;00m(event_loop):\r\n",
      "    \u001b[33m\"\"\"Having a non-async test function, calling loop.run_until_complete\u001b[39;49;00m\r\n",
      "\u001b[33m    with the \"event_loop\" fixture\u001b[39;49;00m\r\n",
      "\u001b[33m    https://github.com/pytest-dev/pytest-asyncio#event_loop\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "    response = event_loop.run_until_complete(get_async())\r\n",
      "    \u001b[34massert\u001b[39;49;00m response.status_code == \u001b[34m200\u001b[39;49;00m\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize examples/G_asyncio/test_G1_async_calls.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.1, pytest-5.3.5, py-1.8.1, pluggy-0.13.1 -- /home/david/.miniconda3/envs/pytest/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/david/Nextcloud/Codigos/Python/2Presentaciones/pyTest\n",
      "plugins: xdist-1.31.0, asyncio-0.10.0, forked-1.1.3, mock-2.0.0\n",
      "collected 2 items                                                              \u001b[0m\n",
      "\n",
      "examples/G_asyncio/test_G1_async_calls.py::test_async_code \u001b[32mPASSED\u001b[0m\n",
      "examples/G_asyncio/test_G1_async_calls.py::test_event_loop \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 4.61s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -sv examples/G_asyncio/test_G1_async_calls.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H) ejecutando tests en paralelo\n",
    "\n",
    "- Ejecutar tests en paralelo puede ser ventajoso cuando nuestros tests tardan un tiempo considerable en ser completados\n",
    "- `pytest-xdist` nos permite ejecutar pytest con un nuevo argumento: `-n auto` ejecutará simultáneamente un test en cada núcleo de CPU que tengamos disponible\n",
    "- Es importante no paralelizar tests (o adaptarlos debidamente) cuando puedan entrar en conflicto entre sí (por ejemplo, cuando hacemos test con una base de datos, ya que podríamos leer o borrar registros en uso por otros tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m\"\"\"H1 - Run multiple tests in parallel using pytest-xdist\u001b[39;49;00m\r\n",
      "\u001b[33mhttps://github.com/pytest-dev/pytest-xdist\u001b[39;49;00m\r\n",
      "\u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpytest\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m sleep\r\n",
      "\r\n",
      "\r\n",
      "\u001b[90m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mtime\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[34m3\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m, \u001b[34m1\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m])\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest_sleep\u001b[39;49;00m(time):\r\n",
      "    \u001b[33m\"\"\"Parametrize 4 tests that will sleep for the given ammount of seconds.\u001b[39;49;00m\r\n",
      "\u001b[33m    When running 4 parallel tests, being 3 the maximum input (sleep time),\u001b[39;49;00m\r\n",
      "\u001b[33m    all tests should run in no more than ~3 seconds.\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "    sleep(time)\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize examples/H_parallelization/test_H1_parallelization.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.1, pytest-5.3.5, py-1.8.1, pluggy-0.13.1 -- /home/david/.miniconda3/envs/pytest/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/david/Nextcloud/Codigos/Python/2Presentaciones/pyTest\n",
      "plugins: xdist-1.31.0, asyncio-0.10.0, forked-1.1.3, mock-2.0.0\n",
      "[gw0] linux Python 3.8.1 cwd: /home/david/Nextcloud/Codigos/Python/2Presentaciones/pyTest\n",
      "[gw1] linux Python 3.8.1 cwd: /home/david/Nextcloud/Codigos/Python/2Presentaciones/pyTest\n",
      "[gw2] linux Python 3.8.1 cwd: /home/david/Nextcloud/Codigos/Python/2Presentaciones/pyTest\n",
      "[gw3] linux Python 3.8.1 cwd: /home/david/Nextcloud/Codigos/Python/2Presentaciones/pyTest\n",
      "[gw4] linux Python 3.8.1 cwd: /home/david/Nextcloud/Codigos/Python/2Presentaciones/pyTest\n",
      "[gw5] linux Python 3.8.1 cwd: /home/david/Nextcloud/Codigos/Python/2Presentaciones/pyTest\n",
      "[gw6] linux Python 3.8.1 cwd: /home/david/Nextcloud/Codigos/Python/2Presentaciones/pyTest\n",
      "[gw7] linux Python 3.8.1 cwd: /home/david/Nextcloud/Codigos/Python/2Presentaciones/pyTest\n",
      "[gw0] Python 3.8.1 (default, Jan  8 2020, 22:29:32)  -- [GCC 7.3.0]1m\n",
      "[gw1] Python 3.8.1 (default, Jan  8 2020, 22:29:32)  -- [GCC 7.3.0][1m\n",
      "[gw2] Python 3.8.1 (default, Jan  8 2020, 22:29:32)  -- [GCC 7.3.0]0m\u001b[1m\n",
      "[gw3] Python 3.8.1 (default, Jan  8 2020, 22:29:32)  -- [GCC 7.3.0]\u001b[0m[1m\u001b[1m\n",
      "[gw4] Python 3.8.1 (default, Jan  8 2020, 22:29:32)  -- [GCC 7.3.0]  \u001b[0m[1m\u001b[1m\n",
      "[gw5] Python 3.8.1 (default, Jan  8 2020, 22:29:32)  -- [GCC 7.3.0]    \u001b[0m\u001b[1m\n",
      "[gw6] Python 3.8.1 (default, Jan  8 2020, 22:29:32)  -- [GCC 7.3.0]      \u001b[0m\u001b[1m\n",
      "[gw7] Python 3.8.1 (default, Jan  8 2020, 22:29:32)  -- [GCC 7.3.0]        \u001b[0m[1m\u001b[1m\n",
      "gw0 [4] / gw1 [4] / gw2 [4] / gw3 [4] / gw4 [4] / gw5 [4] / gw6 [4] / gw7 [4]\u001b[0m[1m\n",
      "scheduling tests via LoadScheduling\n",
      "\n",
      "examples/H_parallelization/test_H1_parallelization.py::test_sleep[31] \n",
      "examples/H_parallelization/test_H1_parallelization.py::test_sleep[1] \n",
      "examples/H_parallelization/test_H1_parallelization.py::test_sleep[30] \n",
      "examples/H_parallelization/test_H1_parallelization.py::test_sleep[2] \n",
      "[gw2] \u001b[32mPASSED\u001b[0m examples/H_parallelization/test_H1_parallelization.py::test_sleep[1] \n",
      "[gw3] \u001b[32mPASSED\u001b[0m examples/H_parallelization/test_H1_parallelization.py::test_sleep[2] \n",
      "[gw1] \u001b[32mPASSED\u001b[0m examples/H_parallelization/test_H1_parallelization.py::test_sleep[31] \n",
      "[gw0] \u001b[32mPASSED\u001b[0m examples/H_parallelization/test_H1_parallelization.py::test_sleep[30] \n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m4 passed\u001b[0m\u001b[32m in 4.00s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -sv -n auto examples/H_parallelization/test_H1_parallelization.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}